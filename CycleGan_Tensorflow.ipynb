{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "CycleGan_Tensorflow.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1f99x9OAPO7vTbgfhz8negK_fw0UXYiZ5",
      "authorship_tag": "ABX9TyMXTwIgzUcjx+2hDVxWdlQV",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "78b08c93ab774262a7eb2bd5aef9f0ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_94f4ae4761b4427b8a6b91e7e780ce46",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_92cf4a3d64844cf9b054810c8324c3c3",
              "IPY_MODEL_f8f21d9e4ff44203ae8584656383fb5f"
            ]
          }
        },
        "94f4ae4761b4427b8a6b91e7e780ce46": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "92cf4a3d64844cf9b054810c8324c3c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_f28f1b3b2bf54987856b4b4f80847379",
            "_dom_classes": [],
            "description": "  1%",
            "_model_name": "FloatProgressModel",
            "bar_style": "danger",
            "max": 1067,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 11,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6b111837710c411484f6a20d10a9cfb3"
          }
        },
        "f8f21d9e4ff44203ae8584656383fb5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_225fbc8106144629ace21af4e1abd500",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 11/1067 [00:06&lt;10:25,  1.69it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_15d49ef4a7ba48deb4716a6603983d5a"
          }
        },
        "f28f1b3b2bf54987856b4b4f80847379": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6b111837710c411484f6a20d10a9cfb3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "225fbc8106144629ace21af4e1abd500": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "15d49ef4a7ba48deb4716a6603983d5a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VedantDere0104/GANs/blob/main/CycleGan_Tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "moyZWV-H-RWi"
      },
      "source": [
        "####"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6yk7kkkFQkI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KivH0OEA-U5z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4f4be3bc-3620-49fa-de8f-0fe30151f395"
      },
      "source": [
        "'''dir = '/content/drive/MyDrive/'\r\n",
        "zip_path = '/content/drive/MyDrive/horse2zebra.zip'\r\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"dir = '/content/drive/MyDrive/'\\nzip_path = '/content/drive/MyDrive/horse2zebra.zip'\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "rjPZmiO2-gNZ",
        "outputId": "5f217762-271e-48a1-e9ab-c21b63c20302"
      },
      "source": [
        "'''!unzip '/content/drive/MyDrive/horse2zebra.zip' -d '/content/drive/MyDrive/'\r\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"!unzip '/content/drive/MyDrive/horse2zebra.zip' -d '/content/drive/MyDrive/'\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmQhef9K-ov7"
      },
      "source": [
        "'''img_domain_1_test = '/content/drive/MyDrive/horse2zebra/testA/'\r\n",
        "img_domain_2_test = '/content/drive/MyDrive/horse2zebra/testB/'\r\n",
        "img_domain_1_train = '/content/drive/MyDrive/horse2zebra/trainA/'\r\n",
        "img_domain_2_train = '/content/drive/MyDrive/horse2zebra/trainB/' \r\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ddq5dDOO_fBv"
      },
      "source": [
        "import os\r\n",
        "import tensorflow as tf\r\n",
        "import numpy as np\r\n",
        "from tqdm.auto import tqdm\r\n",
        "from matplotlib import pyplot\r\n",
        "from numpy.random import randint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmtXRFxX_p7Q"
      },
      "source": [
        "def load_images(paths , size = (256 , 256)):\r\n",
        "  datalist = list()\r\n",
        "\r\n",
        "  for filenames in tqdm(os.listdir(paths)):\r\n",
        "    img = tf.keras.preprocessing.image.load_img(paths + filenames , target_size=size)\r\n",
        "    img = tf.keras.preprocessing.image.img_to_array(img)\r\n",
        "    datalist.append(img)\r\n",
        "  return np.asarray(datalist)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GV3_-E2mAenP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406,
          "referenced_widgets": [
            "78b08c93ab774262a7eb2bd5aef9f0ac",
            "94f4ae4761b4427b8a6b91e7e780ce46",
            "92cf4a3d64844cf9b054810c8324c3c3",
            "f8f21d9e4ff44203ae8584656383fb5f",
            "f28f1b3b2bf54987856b4b4f80847379",
            "6b111837710c411484f6a20d10a9cfb3",
            "225fbc8106144629ace21af4e1abd500",
            "15d49ef4a7ba48deb4716a6603983d5a"
          ]
        },
        "outputId": "c59630f8-e3bf-4129-9cfa-a81874fc9894"
      },
      "source": [
        "domain_1_train = load_images(img_domain_1_train)\r\n",
        "domain_2_train = load_images(img_domain_2_train)\r\n",
        "domain_1_test = load_images(img_domain_1_test)\r\n",
        "domain_2_test = load_images(img_domain_2_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "78b08c93ab774262a7eb2bd5aef9f0ac",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1067.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-184bce35117f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdomain_1_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_domain_1_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdomain_2_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_domain_2_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdomain_1_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_domain_1_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdomain_2_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_domain_2_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-6b6d9bde77e7>\u001b[0m in \u001b[0;36mload_images\u001b[0;34m(paths, size)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mfilenames\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpaths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpaths\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfilenames\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mtarget_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_to_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mdatalist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/preprocessing/image.py\u001b[0m in \u001b[0;36mload_img\u001b[0;34m(path, grayscale, color_mode, target_size, interpolation)\u001b[0m\n\u001b[1;32m    298\u001b[0m   \"\"\"\n\u001b[1;32m    299\u001b[0m   return image.load_img(path, grayscale=grayscale, color_mode=color_mode,\n\u001b[0;32m--> 300\u001b[0;31m                         target_size=target_size, interpolation=interpolation)\n\u001b[0m\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/utils.py\u001b[0m in \u001b[0;36mload_img\u001b[0;34m(path, grayscale, color_mode, target_size, interpolation)\u001b[0m\n\u001b[1;32m    112\u001b[0m                           'The use of `load_img` requires PIL.')\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpil_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcolor_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'grayscale'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0;31m# if image is not already an 8-bit, 16-bit or 32-bit grayscale image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2Of7vU2Ayvy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "10896d37-9401-45c5-bd75-861d75963b32"
      },
      "source": [
        "filename = '/content/drive/MyDrive/horse2zebra/horses2zebras_train.npz'\r\n",
        "'''np.savez_compressed(filename , domain_1_train , domain_2_train)'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'np.savez_compressed(filename , domain_1_train , domain_2_train)'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WfaHYAfSBzMf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "875c662f-d994-46cb-a567-da1512a82922"
      },
      "source": [
        "test_filename = '/content/drive/MyDrive/horse2zebra/horses2zebras_test.npz'\r\n",
        "'''np.savez_compressed(test_filename , domain_1_test , domain_2_test )'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'np.savez_compressed(test_filename , domain_1_test , domain_2_test )'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5mF2RELCxgg"
      },
      "source": [
        "data = np.load(filename)\r\n",
        "data_A , data_B = data['arr_0'] , data['arr_1']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKl0hVy0DJwi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nciq_uKFDYTY"
      },
      "source": [
        "def define_discriminator(input_image_shape):\r\n",
        "  init = tf.keras.initializers.RandomNormal(stddev=0.02)\r\n",
        "  in_src_img = tf.keras.layers.Input(input_shape=input_image_shape)\r\n",
        "  ## C64\r\n",
        "  d = tf.keras.layers.Conv2D(64 , (4 , 4) , strides = (2 , 2) , padding='same' , kernel_initializer=init)(in_src_img)\r\n",
        "  d = tf.keras.layers.LeakyReLU(alpha=0.2)(d)\r\n",
        "  ## C128\r\n",
        "  d = tf.keras.layers.Conv2D(128 , (4 , 4) , strides=(2 , 2) , padding = 'same' , kernel_initializer=init)(d)\r\n",
        "  d = tf.keras.layers.BatchNormalization()(d)\r\n",
        "  d = tf.keras.layers.LeakyReLU(alpha=0.2)(d)\r\n",
        "  ## C256\r\n",
        "  d = tf.keras.layers.Conv2D(256 , (4 , 4) , strides=(2 , 2) , padding='same' , kernel_initializer=init)(d)\r\n",
        "  d = tf.keras.layers.BatchNormalization()(d)\r\n",
        "  d = tf.keras.layers.LeakyReLU(alpha=0.2)(d)\r\n",
        "  ## C512\r\n",
        "  d = tf.keras.layers.Conv2D(512 , (4 , 4) , strides=(2 , 2) , padding='same' , kernel_initializer=init)(d)\r\n",
        "  d = tf.keras.layers.BatchNormalization()(d)\r\n",
        "  d = tf.keras.layers.LeakyReLU(alpha=0.2)(d)\r\n",
        "  ## Second Last\r\n",
        "  d = tf.keras.layers.Conv2D(512 , (4 , 4) , strides=(2 , 2) , padding='same' , kernel_initializer=init)(d)\r\n",
        "  d = tf.keras.layers.BatchNormalization()(d)\r\n",
        "  d = tf.keras.layers.LeakyReLU(alpha=0.2)(d)\r\n",
        "  ## Patch Out\r\n",
        "  patch_out = tf.keras.layers.Conv2D(1 , (4 , 4) , strides=(2 , 2) , padding='same' , kernel_initializer=init)(d)\r\n",
        "  model = tf.keras.Model(in_src_img , patch_out)\r\n",
        "  model.compile(loss = 'mse' , optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002 , beta_1=0.5 ) , loss_weights=[0.5])\r\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwSkbp7HHdDj"
      },
      "source": [
        "def resnet_block(n_filters , input_layer):\r\n",
        "  init = tf.keras.initializers.RandomNormal(stddev=0.02)\r\n",
        "  g = tf.keras.layers.Conv2D(n_filters , (4 , 4) , strides=(2 , 2) , padding='same' , kernel_initializer=init)(input_layer)\r\n",
        "  g = tf.keras.layers.BatchNormalization()(g)\r\n",
        "  g = tf.keras.layers.ReLU()(g)\r\n",
        "  g = tf.keras.layers.Conv2D(n_filters , (4 , 4)  , strides=(2 , 2) , padding='same' , kernel_initializer=init)(g)\r\n",
        "  g = tf.keras.layers.Concatenate()([g , input_layer])\r\n",
        "  return g"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2i91HjsLEiU"
      },
      "source": [
        "def define_generator(input_image_shape , n_resnet = 9):\r\n",
        "  init = tf.keras.initializers.RandomNormal(stddev=0.02)\r\n",
        "  in_src_img = tf.keras.layers.Input(shape=input_image_shape)\r\n",
        "  ## C7-S1-64\r\n",
        "  g = tf.keras.layers.Conv2D(64 , (7 , 7) , strides=(1 , 1) , padding='same' , kernel_initializer=init)(in_src_img)\r\n",
        "  g = tf.keras.layers.BatchNormalization()(g)\r\n",
        "  g = tf.keras.layers.ReLU()(g)\r\n",
        "  ## 128\r\n",
        "  g = tf.keras.layers.Conv2D(128 , (3 , 3) , strides=(2 , 2) , padding='same' , kernel_initializer=init)(g)\r\n",
        "  g = tf.keras.layers.BatchNormalization()(g)\r\n",
        "  g = tf.keras.layers.ReLU()(g)\r\n",
        "  ## 256\r\n",
        "  g = tf.keras.layers.Conv2D(256 , (3 , 3) , strides=(2 , 2) , padding='same' , kernel_initializer=init)(g)\r\n",
        "  g = tf.keras.layers.BatchNormalization()(g)\r\n",
        "  g = tf.keras.layers.ReLU()(g)\r\n",
        "  ## Resnet 256\r\n",
        "  for _ in range(n_resnet):\r\n",
        "    g = resnet_block(256 , g)\r\n",
        "  ## U128\r\n",
        "  g = tf.keras.layers.Conv2DTranspose(128 , (3 , 3) , strides=(2 , 2) , padding='same' , kernel_initializer=init)(g)\r\n",
        "  g = tf.keras.layers.BatchNormalization()(g)\r\n",
        "  g = tf.keras.layers.ReLU()(g)\r\n",
        "  ## U64\r\n",
        "  g = tf.keras.layers.Conv2DTranspose(64 , (3 , 3) , strides=(2 , 2) , padding='same' , kernel_initializer=init)(g)\r\n",
        "  g = tf.keras.layers.BatchNormalization()(g)\r\n",
        "  g = tf.keras.layers.ReLU()(g)\r\n",
        "  ## C7S1-3\r\n",
        "  g = tf.keras.layers.Conv2DTranspose(3 , (7 , 7) , strides=(2 , 2) , padding='same' , kernel_initializer=init)(g)\r\n",
        "  g = tf.keras.layers.BatchNormalization()(g)\r\n",
        "  out_img = tf.keras.layers.Activation('tanh')(g)\r\n",
        "  model = tf.keras.Model(in_src_img , out_img)\r\n",
        "  return model\r\n",
        "  \r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tnk-Cy-rNrtm"
      },
      "source": [
        "def define_composite_model(g_model_1 , d_model , g_model_2 , image_shape):\r\n",
        "  g_model_1.trainable = True\r\n",
        "  d_model.trainable = False\r\n",
        "  g_model_2.trainable = False\r\n",
        "  input_gen = tf.keras.layers.Input(shape=image_shape)\r\n",
        "  gen_1_out = g_model_1(input_gen)\r\n",
        "  output_d = d_model(gen_1_out)\r\n",
        "\r\n",
        "  input_id = tf.keras.layers.Input(shape = image_shape)\r\n",
        "  output_id = g_model_1(input_id)\r\n",
        "  ## Forward Cycle\r\n",
        "  output_f = g_model_2(gen_1_out)\r\n",
        "  ## Backwad Cycle\r\n",
        "  gen_2_out = g_model_2(input_id)\r\n",
        "  output_b = g_model_1(gen_2_out)\r\n",
        "\r\n",
        "  model = tf.keras.Model([input_gen , input_id] , [output_d , output_id , output_f , output_b])\r\n",
        "  opt = tf.keras.optimizers.Adam(learning_rate=0.0002 , beta_1=0.5)\r\n",
        "  model.compile(loss = ['mse' , 'mae' , 'mae' , 'mae'] , loss_weights = [1 , 5 , 10 , 10] , optimizer = opt)\r\n",
        "  return model\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YN6jrqmB_zQf"
      },
      "source": [
        "def load_real_samples(filename):\r\n",
        "  data = np.load(filename)\r\n",
        "  X1 , X2 = data['arr_0'] , data['arr_1']\r\n",
        "  X1 = (X1 - 127.5) / 127.5\r\n",
        "  X2 = (X2 - 127.5) / 127.5\r\n",
        "  return [X1 , X2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RxHJZYCQAV_M"
      },
      "source": [
        "def generate_real_samples(dataset , n_samples , patch_shape):\r\n",
        "  ix = randint(0 , dataset.shape[0] , n_samples)\r\n",
        "  X = dataset[ix]\r\n",
        "  y = np.ones((n_samples , patch_shape , patch_shape , 1))\r\n",
        "  return X , y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZnaO0uryA22I"
      },
      "source": [
        "def generate_fake_samples(g_model , dataset , patch_shape):\r\n",
        "  X = g_model.predict(dataset)\r\n",
        "  y = np.zeros((len(X) , patch_shape , patch_shape , 1))\r\n",
        "  return X , y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w68x6EyRBJPZ"
      },
      "source": [
        "def save_model(step , g_model_A_to_B , g_model_B_to_A):\r\n",
        "  filename_1 = 'g_model_A_to_B.h5'\r\n",
        "  g_model_A_to_B.save(filename_1)\r\n",
        "  filename_2 = 'g_model_B_to_A.h5'\r\n",
        "  g_model_B_to_A.save(filename_2)\r\n",
        "  print('>Saved: %s and %s' % (filename1, filename2))\r\n",
        "  \r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uwkesdr6BwgZ"
      },
      "source": [
        "def summarize_performance(step , g_model , train_X , name , n_samples = 5):\r\n",
        "  X_in , _ = generate_real_samples(train_X , n_samples , 0)\r\n",
        "  X_out , _ = generate_fake_samples(g_model , X_in , 0)\r\n",
        "  X_in = (X_in + 1) / 2.0\r\n",
        "  X_out = (X_out + 1) / 2.0\r\n",
        "  for i in range(n_samples):\r\n",
        "    pyplot.subplot(2, n_samples, 1 + i)\r\n",
        "    pyplot.axis('off')\r\n",
        "    pyplot.imshow(X_in[i])\r\n",
        "# plot translated image\r\n",
        "  for i in range(n_samples):\r\n",
        "    pyplot.subplot(2, n_samples, 1 + n_samples + i)\r\n",
        "    pyplot.axis('off')\r\n",
        "    pyplot.imshow(X_out[i])\r\n",
        "# save plot to file\r\n",
        "  filename1 = '%s_generated_plot_%06d.png' % (name, (step+1))\r\n",
        "  pyplot.savefig(filename1)\r\n",
        "  pyplot.close()\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eY0C64h2Cce0"
      },
      "source": [
        "def update_image_pool(pool, images, max_size=50):\r\n",
        "  selected = list()\r\n",
        "  for image in images:\r\n",
        "    if len(pool) < max_size:\r\n",
        "      # stock the pool\r\n",
        "      pool.append(image)\r\n",
        "      selected.append(image)\r\n",
        "    elif random() < 0.5:\r\n",
        "      # use image, but donâœ¬t add it to the pool\r\n",
        "      selected.append(image)\r\n",
        "    else:\r\n",
        "  # replace an existing image and use replaced image\r\n",
        "      ix = randint(0, len(pool))\r\n",
        "      selected.append(pool[ix])\r\n",
        "      pool[ix] = image\r\n",
        "  return asarray(selected)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8LRmxU4Cwv0"
      },
      "source": [
        "def train(d_model_A , d_model_B , g_model_A_to_B , g_model_B_to_A , c_model_A_to_B , c_model_B_to_A , dataset):\r\n",
        "  n_epochs , n_batch = 100 , 1\r\n",
        "  n_patch = d_model_A.output_shape[1]\r\n",
        "  train_A , train_B = dataset\r\n",
        "  pool_A , pool_B = list() , list()\r\n",
        "  batch_per_epoch = int(len(train_A)/n_batch)\r\n",
        "  n_steps = batch_per_epoch * n_epochs\r\n",
        "  \r\n",
        "  for i in range(n_steps):\r\n",
        "    X_real_A , y_real_A = generate_real_samples(train_A , n_batch , n_patch)\r\n",
        "    X_real_B , y_real_B = generate_real_samples(train_B , n_batch , n_patch)\r\n",
        "\r\n",
        "    X_fake_A , y_fake_A = generate_fake_samples(g_model_B_to_A , X_real_B , n_patch)\r\n",
        "    X_fake_B , y_fake_B = generate_fake_samples(g_model_A_to_B , X_real_A , n_patch)\r\n",
        "    X_fake_A = update_image_pool(pool_A , X_fake_A)\r\n",
        "    X_fake_B = update_image_pool(pool_B , X_fake_B)\r\n",
        "\r\n",
        "    g_loss_2 , _ , _ , _ , _ = c_model_B_to_A.train_on_batch([X_real_B , X_real_A] , [y_real_A , X_real_A , X_real_B , X_real_A])\r\n",
        "    dB_loss1 = d_model_B.train_on_batch(X_real_B , y_real_B)\r\n",
        "    dB_loss2 = d_model_B.train_on_batch(X_fake_B , y_fake_B)\r\n",
        "    print('>%d, dA[%.3f,%.3f] dB[%.3f,%.3f] g[%.3f,%.3f]' % (i+1, dA_loss1,dA_loss2,dB_loss1,dB_loss2, g_loss1,g_loss2))\r\n",
        "    if (i+1) % (bat_per_epo * 1) == 0:\r\n",
        "      # plot A->B translation\r\n",
        "      summarize_performance(i, g_model_AtoB, trainA, 'AtoB')\r\n",
        "      # plot B->A translation\r\n",
        "      summarize_performance(i, g_model_BtoA, trainB, 'BtoA')\r\n",
        "    if (i+1) % (bat_per_epo * 5) == 0:\r\n",
        "      #  save the models\r\n",
        "      save_models(i, g_model_AtoB, g_model_BtoA)\r\n",
        "    \r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ol_h8DgHGw9x"
      },
      "source": [
        "dataset = load_real_samples(filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v5heXUckG5nh",
        "outputId": "00541c81-feac-43c3-eb34-55f509b2d851"
      },
      "source": [
        "print(dataset[0].shape , dataset[1].shape)\r\n",
        "image_shape = dataset[0].shape[1:]\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1067, 256, 256, 3) (1334, 256, 256, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlGH1bpSG9nw"
      },
      "source": [
        "g_model_A_to_B = define_generator(image_shape)\r\n",
        "g_model_B_to_A = define_generator(image_shape)\r\n",
        "d_model_A = define_discriminator(image_shape)\r\n",
        "d_model_B = define_discriminator(image_shape)\r\n",
        "c_model_A_to_B = define_composite_model(g_model_A_to_B , d_model_B , g_model_B_to_A , image_shape)\r\n",
        "c_model_B_to_A = define_composite_model(g_model_B_to_A , d_model_A , g_model_A_to_B , image_shape)\r\n",
        "train(d_model_A, d_model_B, g_model_AtoB, g_model_BtoA, c_model_AtoB, c_model_BtoA, dataset)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDCTZNfBIBVH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}