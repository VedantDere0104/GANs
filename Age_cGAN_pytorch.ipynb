{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Age-cGAN_pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOwkKekCogceEpCGZ0Sj0xx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VedantDere0104/GANs/blob/main/Age_cGAN_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_PxS4CmuOPh"
      },
      "source": [
        "Face Aging With Conditional Generative Adversarial Networks :- https://arxiv.org/abs/1702.01983"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FyPCe0mRTCpD"
      },
      "source": [
        "####"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yjdxf0AiTMkP"
      },
      "source": [
        "import torch\r\n",
        "from torch import nn\r\n",
        "import torchvision\r\n",
        "from torchsummary import summary\r\n",
        "from tqdm.auto import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GiJyA6STQEW"
      },
      "source": [
        "!wget https://data.vision.ee.ethz.ch/cvl/rrothe/imdb-wiki/static/wiki_crop.tar\r\n",
        "!tar -xf wiki_crop.tar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8AGGgBm3TRtf"
      },
      "source": [
        "import math\r\n",
        "import os\r\n",
        "import time\r\n",
        "from datetime import datetime\r\n",
        "from pathlib import Path\r\n",
        "from glob import glob\r\n",
        "\r\n",
        "import pickle as pkl\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import numpy as np\r\n",
        "from scipy.io import loadmat\r\n",
        "from PIL import Image\r\n",
        "\r\n",
        "import torch\r\n",
        "from torch.utils.data import Dataset, DataLoader\r\n",
        "from torchvision import datasets, transforms, utils\r\n",
        "\r\n",
        "%matplotlib inline\r\n",
        "import warnings\r\n",
        "warnings.filterwarnings('ignore')\r\n",
        "\r\n",
        "%matplotlib inline\r\n",
        "\r\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6d8JA4jjXMkE"
      },
      "source": [
        "def show_tensor_images(image_tensor, num_images=25, size=(1, 28, 28)):\r\n",
        "\r\n",
        "    image_tensor = (image_tensor + 1) / 2\r\n",
        "    image_unflat = image_tensor.detach().cpu()\r\n",
        "    image_grid = make_grid(image_unflat[:num_images], nrow=5)\r\n",
        "    plt.imshow(image_grid.permute(1, 2, 0).squeeze())\r\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbJNdA9CTS_k"
      },
      "source": [
        "def calc_age(taken, dob):\r\n",
        "\r\n",
        "    birth = datetime.fromordinal(max(int(dob) - 366, 1))\r\n",
        "\r\n",
        "  \r\n",
        "    if birth.month < 7:\r\n",
        "        return taken - birth.year\r\n",
        "    else:\r\n",
        "        return taken - birth.year - 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_5DVSPSTUTf"
      },
      "source": [
        "def load_data(dataset='wiki', data_dir='./wiki_crop'):\r\n",
        "\r\n",
        "    meta_path = Path(data_dir) / f'{dataset}.mat'\r\n",
        "    meta = loadmat(meta_path)\r\n",
        "    meta_data = meta[dataset][0, 0]\r\n",
        "\r\n",
        "    full_path = meta_data['full_path'][0]\r\n",
        "    full_path = [y for x in full_path for y in x]\r\n",
        "\r\n",
        "\r\n",
        "    dob = meta_data['dob'][0]\r\n",
        "\r\n",
        "\r\n",
        "    photo_taken = meta_data['photo_taken'][0]\r\n",
        "\r\n",
        "    age = [calc_age(photo_taken[i], dob[i]) for i in range(len(dob))]\r\n",
        "\r\n",
        "    clean_mapping = {pth:age for (pth, age) in zip(full_path, age) if age > 0}\r\n",
        "    \r\n",
        "\r\n",
        "    full_path = list(clean_mapping.keys())\r\n",
        "    age = list(clean_mapping.values())\r\n",
        "\r\n",
        "    return full_path, age"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jThQmsoqTVfe"
      },
      "source": [
        "def scale(x, feature_range=(-1, 1)):\r\n",
        "\r\n",
        "    min, max = feature_range\r\n",
        "    x = x * (max - min) + min\r\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QeLN6h1RTXsX"
      },
      "source": [
        "bins = [18, 29, 39, 49, 59]\r\n",
        "def one_hot(x, bins):\r\n",
        "\r\n",
        "    x = x.numpy()\r\n",
        "    idxs = np.digitize(x, bins, right=True)\r\n",
        "    idxs = idxs.reshape(-1,1)\r\n",
        "    z = torch.zeros(len(x), len(bins)+1).scatter_(1, torch.tensor(idxs), 1)\r\n",
        "    return z"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fws_nwieTZWh"
      },
      "source": [
        "class ImageAgeDataset(Dataset):\r\n",
        "\r\n",
        "    def __init__(self, dataset, data_dir, transform=None):\r\n",
        "\r\n",
        "        self.data_dir = data_dir\r\n",
        "        self.full_path, self.age = load_data(dataset, data_dir)\r\n",
        "        self.transform = transform\r\n",
        "    \r\n",
        "    def __len__(self):\r\n",
        "        return len(self.age)\r\n",
        "    \r\n",
        "    def __getitem__(self, idx):\r\n",
        "        image = Image.open(os.path.join(self.data_dir, self.full_path[idx]))\r\n",
        "        age = self.age[idx]\r\n",
        "        sample = {'image': image, 'age': age}\r\n",
        "        if self.transform:\r\n",
        "            sample = self.transform(sample)\r\n",
        "        return sample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdJMGrfhTawX"
      },
      "source": [
        "class Resize(object):\r\n",
        "    \r\n",
        "    \r\n",
        "    def __init__(self, output_size):\r\n",
        "        assert isinstance(output_size, (int, tuple))\r\n",
        "        self.output_size = output_size\r\n",
        "        \r\n",
        "    def __call__(self, sample):\r\n",
        "        image, age = sample['image'], sample['age']\r\n",
        "        image = transforms.Resize(self.output_size)(image)\r\n",
        "        return {'image': image, 'age': age}\r\n",
        "\r\n",
        "class ToTensor(object):\r\n",
        "\r\n",
        "\r\n",
        "    def __call__(self, sample):\r\n",
        "        image, age = sample['image'], sample['age']\r\n",
        "        image = transforms.ToTensor()(image)\r\n",
        "        # expand dept from 1 to 3 channels for gray images\r\n",
        "        if image.size()[0] == 1:\r\n",
        "            image = image.expand(3,-1,-1)\r\n",
        "        return {'image': image, 'age': age}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIHD588KTb9h"
      },
      "source": [
        "dataset='wiki'\r\n",
        "data_dir='./wiki_crop'\r\n",
        "bins = [18, 29, 39, 49, 59]\r\n",
        "img_size = 256\r\n",
        "batch_size = 128\r\n",
        "#num_workers = 0\r\n",
        "\r\n",
        "tfms = transforms.Compose([Resize((img_size, img_size)),\r\n",
        "                           ToTensor()])\r\n",
        "\r\n",
        "train_dataset = ImageAgeDataset(dataset, data_dir, transform=tfms)\r\n",
        "\r\n",
        "# build DataLoaders\r\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7BJjzhQTdeA"
      },
      "source": [
        "iter(train_loader).next()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2nN2rNhTeov"
      },
      "source": [
        "# obtain one batch of training images\r\n",
        "dataiter = iter(train_loader)\r\n",
        "data = dataiter.next()\r\n",
        "images, labels = data['image'], data['age']\r\n",
        "\r\n",
        "# plot the images in the batch, along with the corresponding labels\r\n",
        "fig = plt.figure(figsize=(25, 4))\r\n",
        "plot_size=20\r\n",
        "for idx in np.arange(plot_size):\r\n",
        "    ax = fig.add_subplot(2, plot_size/2, idx+1, xticks=[], yticks=[])\r\n",
        "    ax.imshow(np.transpose(images[idx], (1, 2, 0)))\r\n",
        "    # print out the correct label for each image\r\n",
        "    # .item() gets the value contained in a Tensor\r\n",
        "    ax.set_title(str(labels[idx].item()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-uWwZKyThAp"
      },
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Bqnk4GzTNyH"
      },
      "source": [
        "class Encoder(nn.Module):\r\n",
        "  def __init__(self , image_in_channels , z_dim):\r\n",
        "    super(Encoder , self).__init__()\r\n",
        "\r\n",
        "    n_filters = 64\r\n",
        "\r\n",
        "    self.conv1 = nn.Conv2d(image_in_channels , n_filters , kernel_size= 4 , stride=2 )\r\n",
        "    self.batch_norm1 = nn.BatchNorm2d(n_filters)\r\n",
        "\r\n",
        "    self.conv2 = nn.Conv2d(n_filters , n_filters * 2 , kernel_size=4 , stride=2)\r\n",
        "    self.batch_norm2 = nn.BatchNorm2d(n_filters * 2)\r\n",
        "\r\n",
        "    self.conv3 = nn.Conv2d(n_filters *2 , n_filters * 4 , kernel_size=4 , stride=2)\r\n",
        "    self.batch_norm3 = nn.BatchNorm2d(n_filters * 4)\r\n",
        "\r\n",
        "    self.conv4 = nn.Conv2d(n_filters * 4 , n_filters * 8 , kernel_size=4 , stride=2)\r\n",
        "    self.batch_norm4 = nn.BatchNorm2d(n_filters * 8)\r\n",
        "\r\n",
        "    self.conv5 = nn.Conv2d(n_filters * 8 , n_filters * 16 , kernel_size=4 , stride=2)\r\n",
        "    self.batch_norm5 = nn.BatchNorm2d(n_filters * 16)\r\n",
        "\r\n",
        "    self.relu = nn.ReLU(inplace=True)\r\n",
        "\r\n",
        "    self.flatten = nn.Flatten()\r\n",
        "\r\n",
        "    self.linear1 = nn.Linear(36864 , n_filters * 2)\r\n",
        "    self.batch_norm_linear1 = nn.BatchNorm1d(n_filters * 2)\r\n",
        "    self.linear2 = nn.Linear(n_filters * 2 , n_filters)\r\n",
        "    self.batch_norm_linear2 = nn.BatchNorm1d(n_filters)\r\n",
        "    self.linear3 = nn.Linear(n_filters , z_dim)\r\n",
        "\r\n",
        "\r\n",
        "  def forward(self , x):\r\n",
        "    x = self.conv1(x)\r\n",
        "    x = self.batch_norm1(x)\r\n",
        "    x = self.relu(x)\r\n",
        "\r\n",
        "    x = self.conv2(x)\r\n",
        "    x = self.batch_norm2(x)\r\n",
        "    x = self.relu(x)\r\n",
        "\r\n",
        "    x = self.conv3(x)\r\n",
        "    x = self.batch_norm3(x)\r\n",
        "    x = self.relu(x)\r\n",
        "\r\n",
        "    x = self.conv4(x)\r\n",
        "    x = self.batch_norm4(x)\r\n",
        "    x = self.relu(x)\r\n",
        "\r\n",
        "    x = self.conv5(x)\r\n",
        "    x = self.batch_norm5(x)\r\n",
        "    x = self.relu(x)\r\n",
        "\r\n",
        "    x = self.flatten(x)\r\n",
        "\r\n",
        "    x = self.linear1(x)\r\n",
        "    x = self.batch_norm_linear1(x)\r\n",
        "    x = self.relu(x)\r\n",
        "    \r\n",
        "    x = self.linear2(x)\r\n",
        "    x = self.batch_norm_linear2(x)\r\n",
        "    x = self.relu(x)\r\n",
        "\r\n",
        "    x = self.linear3(x)\r\n",
        "\r\n",
        "    return x\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mlqOmqCaYBw"
      },
      "source": [
        "encoder = Encoder(3 , 100)\r\n",
        "encoder = encoder.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBZHUPBdabpd"
      },
      "source": [
        "summary(encoder , (3 , 256 , 256))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rN1uMyBTvT-"
      },
      "source": [
        "class Identity_preserving_optimization(nn.Module):\r\n",
        "  def __init__(self , z_dim , z_star_dim):\r\n",
        "    super(Identity_preserving_optimization , self).__init__()\r\n",
        "    \r\n",
        "    n_filters = 64\r\n",
        "\r\n",
        "    self.linear1 = nn.Linear(z_dim , n_filters)\r\n",
        "    self.batch_norm1 = nn.BatchNorm1d(n_filters)\r\n",
        "    \r\n",
        "    self.linear2 = nn.Linear(n_filters , n_filters * 2)\r\n",
        "    self.batch_norm2  = nn.BatchNorm1d(n_filters * 2)\r\n",
        "\r\n",
        "    self.linear3 = nn.Linear(n_filters * 2 , n_filters * 4)\r\n",
        "    self.batch_norm3 = nn.BatchNorm1d(n_filters * 4)\r\n",
        "\r\n",
        "    self.linear4 = nn.Linear(n_filters * 4 , n_filters * 8)\r\n",
        "    self.batch_norm4 = nn.BatchNorm1d(n_filters * 8)\r\n",
        "\r\n",
        "    self.linear5 = nn.Linear(n_filters * 8 , z_star_dim)\r\n",
        "    \r\n",
        "    self.relu = nn.ReLU()\r\n",
        "\r\n",
        "  def forward(self , x):\r\n",
        "    x = self.linear1(x)\r\n",
        "    x = self.batch_norm1(x)\r\n",
        "    x = self.relu(x)\r\n",
        "\r\n",
        "    x = self.linear2(x)\r\n",
        "    x = self.batch_norm2(x)\r\n",
        "    x = self.relu(x)\r\n",
        "\r\n",
        "    x = self.linear3(x)\r\n",
        "    x = self.batch_norm3(x)\r\n",
        "    x = self.relu(x)\r\n",
        "\r\n",
        "    x = self.linear4(x)\r\n",
        "    x = self.batch_norm4(x)\r\n",
        "    x = self.relu(x)\r\n",
        "\r\n",
        "    x = self.linear5(x)\r\n",
        "    return x\r\n",
        "\r\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jExU9mEmVxxI"
      },
      "source": [
        "class conv(nn.Module):\r\n",
        "  def __init__(self , in_channels , out_channels , kernel_size = 4 , strides = 2 , padding = 1 , batch_norm = True):\r\n",
        "    super(conv , self).__init__()\r\n",
        "\r\n",
        "    self.conv_layer = nn.Conv2d(in_channels , out_channels , kernel_size , strides , padding)\r\n",
        "    self.batch_norm = batch_norm\r\n",
        "\r\n",
        "    if self.batch_norm:\r\n",
        "      self.batchnorm = nn.BatchNorm2d(out_channels)\r\n",
        "    \r\n",
        "  def forward(self , x):\r\n",
        "    x = self.conv_layer(x)\r\n",
        "    if self.batch_norm:\r\n",
        "      x = self.batchnorm(x)\r\n",
        "    \r\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ylHGoa7QVh-C"
      },
      "source": [
        "class Discriminator(nn.Module):\r\n",
        "  def __init__(self , y_size , conv_dim = 64):\r\n",
        "    super(Discriminator , self).__init__()\r\n",
        "\r\n",
        "    self.y_size = y_size\r\n",
        "    self.conv_dim = conv_dim\r\n",
        "\r\n",
        "    self.conv1 = nn.Conv2d(3 , 58 , kernel_size=2 , stride=2 )\r\n",
        "    self.relu = nn.ReLU(inplace=True)\r\n",
        "\r\n",
        "    self.seq = nn.Sequential(\r\n",
        "        \r\n",
        "        nn.Conv2d(58 + y_size , conv_dim * 2 , kernel_size=2 , stride=2) , \r\n",
        "        nn.BatchNorm2d(conv_dim * 2) , \r\n",
        "        nn.ReLU(inplace=True) , \r\n",
        "\r\n",
        "        nn.Conv2d(conv_dim * 2 , conv_dim * 4 , kernel_size=2 , stride=2) , \r\n",
        "        nn.BatchNorm2d(conv_dim  *4) , \r\n",
        "        nn.ReLU(inplace=True) , \r\n",
        "\r\n",
        "        nn.Conv2d(conv_dim * 4 , conv_dim * 16 , kernel_size=2 , stride=2 ) , \r\n",
        "        nn.BatchNorm2d(conv_dim * 16) , \r\n",
        "        nn.ReLU(inplace=True) , \r\n",
        "\r\n",
        "        nn.Conv2d(conv_dim * 16 , conv_dim * 32 , kernel_size=2 , stride=2) , \r\n",
        "        nn.BatchNorm2d(conv_dim * 32) , \r\n",
        "        nn.ReLU(inplace=True) , \r\n",
        "\r\n",
        "        nn.Conv2d(conv_dim * 32 , conv_dim * 64 , kernel_size=2 , stride=2 ) , \r\n",
        "        nn.BatchNorm2d(conv_dim * 64) , \r\n",
        "        nn.ReLU(inplace=True) , \r\n",
        "\r\n",
        "        nn.Conv2d(conv_dim * 64 , conv_dim , kernel_size=2 , stride=2) ,  \r\n",
        "        nn.BatchNorm2d(conv_dim),\r\n",
        "        nn.ReLU(inplace=True) , \r\n",
        "\r\n",
        "        nn.Conv2d(conv_dim, 1, kernel_size=2 , stride=2 , padding = 0) ,  \r\n",
        "        #nn.ReLU(inplace=True) , \r\n",
        "\r\n",
        ")\r\n",
        "\r\n",
        "  def forward(self , x , y):\r\n",
        "    x = self.conv1(x)\r\n",
        "    x = self.relu(x)\r\n",
        "    y = y.view(-1,y.size()[-1],1,1)\r\n",
        "    y = y.expand(-1,-1,x.size()[-2], x.size()[-1])\r\n",
        "    x = torch.cat([x, y], 1)\r\n",
        "    x = self.seq(x)\r\n",
        "    \r\n",
        "    return x\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IcONfhevih3k"
      },
      "source": [
        "disc = Discriminator(6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5GHfTZMij6r"
      },
      "source": [
        "\r\n",
        "real_images_ = torch.randn(32 , 3 , 256 , 256)\r\n",
        "ages_ = torch.randn(32 , 6)\r\n",
        "\r\n",
        "_ = disc(real_images_ , ages_)\r\n",
        "_.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmj8EFDiNrH8"
      },
      "source": [
        "#discriminator_ = Discriminator_(100)\r\n",
        "#discriminator_ = discriminator_.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KI140SNONvE-"
      },
      "source": [
        "#summary(discriminator_ , (3 , 256 , 256))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4crYW0YwVus6"
      },
      "source": [
        "  class deconv(nn.Module):\r\n",
        "    def __init__(self , in_channels , out_channels , kernel_size = 4 , stride = 2 , padding = 1 , batch_norm = True):\r\n",
        "      super(deconv , self).__init__()\r\n",
        "\r\n",
        "      self.convT = nn.ConvTranspose2d(in_channels , out_channels , kernel_size , stride , padding)\r\n",
        "      self.batch_norm = batch_norm\r\n",
        "\r\n",
        "      if self.batch_norm:\r\n",
        "        self.batchnorm = nn.BatchNorm2d(out_channels)\r\n",
        "    \r\n",
        "    def forward(self , x):\r\n",
        "      x = self.convT(x)\r\n",
        "      if self.batch_norm:\r\n",
        "        x = self.batchnorm(x)\r\n",
        "      return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t26fzN3GV07Y"
      },
      "source": [
        "class Generator(nn.Module):\r\n",
        "  def __init__(self , z_dim , y_dim , conv_dim = 64):\r\n",
        "    super(Generator , self).__init__()\r\n",
        "\r\n",
        "    self.deconv1 = deconv(z_dim + y_dim , conv_dim * 8 , kernel_size = 4 , stride=2 , padding = 0)\r\n",
        "    self.deconv2 = deconv(conv_dim * 8 , conv_dim * 16 , 4)\r\n",
        "    self.deconv3 = deconv(conv_dim * 16 , conv_dim * 8)\r\n",
        "    self.deconv4 = deconv(conv_dim * 8 , conv_dim * 8)\r\n",
        "    self.deconv5 = deconv(conv_dim * 8 , conv_dim * 4 , 4 ,)\r\n",
        "    self.deconv6 = deconv(conv_dim * 4 , conv_dim * 2 , 4 , 2 , 1)\r\n",
        "    self.deconv7 = deconv(conv_dim * 2 , 3 , 4 , 2 , 1 , batch_norm=False)\r\n",
        "    self.relu = nn.ReLU(inplace=True)\r\n",
        "\r\n",
        "  def forward(self , z , y):\r\n",
        "    x = torch.cat([z , y] , dim = 1)\r\n",
        "    x = x.view(-1, x.size()[-1], 1, 1)\r\n",
        "    x = self.relu(self.deconv1(x))\r\n",
        "    x = self.relu(self.deconv2(x))\r\n",
        "    x = self.relu(self.deconv3(x))\r\n",
        "    x = self.relu(self.deconv4(x))\r\n",
        "    x = self.relu(self.deconv5(x))\r\n",
        "    x = self.relu(self.deconv6(x))\r\n",
        "    x = self.deconv7(x)\r\n",
        "    x = torch.tanh(x)\r\n",
        "    #x = torch.reshape(x , (x.shape[0] , 3 , 256 , 256))\r\n",
        "    return x\r\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwOxWIcydxR7"
      },
      "source": [
        "generator = Generator(100 , 6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2hAcZsZoaGJ"
      },
      "source": [
        "z = torch.randn(32 , 100)\r\n",
        "y = torch.randn(32 , 6)\r\n",
        "x = generator(z , y)\r\n",
        "x.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQAtYjjed1Ra"
      },
      "source": [
        "\r\n",
        "print(generator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNjR8q8ZV2M6"
      },
      "source": [
        "conv_dim = 64\r\n",
        "z_size = 100\r\n",
        "z_star_size = 100\r\n",
        "y_size = 6\r\n",
        "image_in_channels = 3\r\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\r\n",
        "\r\n",
        "encoder = Encoder(image_in_channels , z_size)\r\n",
        "identity_preserving_optimizer = Identity_preserving_optimization(z_size , z_star_size)\r\n",
        "discriminator = Discriminator(y_size , conv_dim=conv_dim)\r\n",
        "generator = Generator(z_size , y_size , conv_dim)\r\n",
        "\r\n",
        "encoder = encoder.to(device)\r\n",
        "identity_preserving_optimizer = identity_preserving_optimizer.to(device)\r\n",
        "discriminator = discriminator.to(device)\r\n",
        "generator = generator.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8FnuKYBWBx9"
      },
      "source": [
        "print(encoder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bSlHHQbdW7Qa"
      },
      "source": [
        "print(identity_preserving_optimizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arvRs5fjW-5s"
      },
      "source": [
        "print(generator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cyn8-7JAXBkx"
      },
      "source": [
        "print(discriminator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUWIpl9NXC3Y"
      },
      "source": [
        "def real_loss(d_out , smooth = False):\r\n",
        "  batch_size = d_out.size(0)\r\n",
        "  if smooth:\r\n",
        "    labels = torch.ones(batch_size) * 0.9\r\n",
        "\r\n",
        "  else:\r\n",
        "    labels = torch.ones(batch_size)\r\n",
        "\r\n",
        "  labels = labels.to(device)\r\n",
        "  criterion = nn.BCEWithLogitsLoss()\r\n",
        "  loss = criterion(d_out.squeeze() , labels)\r\n",
        "  return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k8OxV-K_XTk0"
      },
      "source": [
        "\r\n",
        "def fake_loss(d_out):\r\n",
        "  batch_size = d_out.size(0)\r\n",
        "  labels = torch.zeros(batch_size)\r\n",
        "  labels = labels.to(device)\r\n",
        "\r\n",
        "  criterion = nn.BCEWithLogitsLoss()\r\n",
        "  loss = criterion(d_out.squeeze() , labels)\r\n",
        "  return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0uUPJf8EXUoL"
      },
      "source": [
        "lr = 0.0002\r\n",
        "beta1=0.5\r\n",
        "beta2=0.999\r\n",
        "\r\n",
        "\r\n",
        "num_epochs = 50\r\n",
        "\r\n",
        "d_optimizer = torch.optim.Adam(discriminator.parameters() , lr = lr , betas=(beta1 , beta2))\r\n",
        "g_optimizer = torch.optim.Adam(generator.parameters() , lr = lr , betas=(beta1 , beta2))\r\n",
        "i_optimizer = torch.optim.Adam(identity_preserving_optimizer.parameters() , lr = lr , betas=(beta1 , beta2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "keFQpHw_XV-i"
      },
      "source": [
        "def checkpoint(G, D, epoch, model, root_dir):\r\n",
        "    target_dir = f'{root_dir}/{model}'\r\n",
        "    os.makedirs(target_dir, exist_ok=True)\r\n",
        "    G_path = os.path.join(target_dir, f'G_{epoch}.pkl')\r\n",
        "    D_path = os.path.join(target_dir, f'D_{epoch}.pkl')\r\n",
        "    torch.save(G.state_dict(), G_path)\r\n",
        "    torch.save(D.state_dict(), D_path)\r\n",
        "\r\n",
        "def oh_to_class(fixed_y):\r\n",
        "    age_map = {0:'0-18',1:'19-29',2:'30-39',3:'40-49',4:'50-59',5:'60+'}\r\n",
        "    if torch.cuda.is_available():\r\n",
        "        fixed_y = fixed_y.cpu()\r\n",
        "    fixed_y_idxs = fixed_y.numpy().nonzero()[1]\r\n",
        "    fixed_y_ages = [age_map[idx] for idx in fixed_y_idxs]\r\n",
        "    \r\n",
        "    return fixed_y_ages\r\n",
        "\r\n",
        "def save_samples_ages(samples, fixed_y, model, root_dir):\r\n",
        "    fixed_y_ages = oh_to_class(fixed_y)\r\n",
        "    samples_ages = {'samples': samples, 'ages': fixed_y_ages}\r\n",
        "    target_dir = f'{root_dir}/{model}'\r\n",
        "    os.makedirs(target_dir, exist_ok=True)\r\n",
        "    with open(f'{target_dir}/train_samples_ages.pkl', 'wb') as f:\r\n",
        "        pkl.dump(samples_ages, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-VBR4dzXXOx"
      },
      "source": [
        "%%time\r\n",
        "\r\n",
        "root_dir = '/content/Age-cGAN'\r\n",
        "model = 'GAN_1'\r\n",
        "os.makedirs(root_dir, exist_ok=True)\r\n",
        "\r\n",
        "\r\n",
        "generator.to(device)\r\n",
        "discriminator.to(device)\r\n",
        "\r\n",
        "import pickle as pkl\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "samples = []\r\n",
        "losses = []\r\n",
        "\r\n",
        "print_every = 300\r\n",
        "\r\n",
        "\r\n",
        "sample_size=16\r\n",
        "fixed_z = np.random.uniform(-1, 1, size=(sample_size, z_size))\r\n",
        "fixed_z = torch.from_numpy(fixed_z).float()\r\n",
        "fixed_y = np.random.randint(len(bins), size=sample_size)\r\n",
        "fixed_y = fixed_y.reshape(-1,1)\r\n",
        "fixed_y = torch.zeros(sample_size, len(bins)+1).scatter_(1, torch.tensor(fixed_y), 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mONtQItGXZHs"
      },
      "source": [
        "for epoch in range(num_epochs):\r\n",
        "  \r\n",
        "  for batch_i, batch in enumerate(tqdm(train_loader)):\r\n",
        "\r\n",
        "    batch_size = batch['image'].size(0)\r\n",
        "\r\n",
        "    real_images = scale(batch['image'])\r\n",
        "\r\n",
        "    ages = one_hot(batch['age'], bins)\r\n",
        "    #print(ages.shape)\r\n",
        "\r\n",
        "    real_images = real_images.to(device)\r\n",
        "    ages = ages.to(device)\r\n",
        "\r\n",
        "    z = encoder(real_images)\r\n",
        "    z = z.to(device)\r\n",
        "\r\n",
        "    d_optimizer.zero_grad()\r\n",
        "\r\n",
        "\r\n",
        "    #print(real_images.shape , ages.shape)\r\n",
        "\r\n",
        "    D_real = discriminator(real_images , ages)\r\n",
        "    d_real_loss = real_loss(D_real)\r\n",
        "\r\n",
        "\r\n",
        "    fake_images = generator(z, ages)\r\n",
        "    D_fake = discriminator(fake_images , ages)\r\n",
        "    d_fake_loss = fake_loss(D_fake)\r\n",
        "    d_loss = d_real_loss + d_fake_loss\r\n",
        "    d_loss.backward()\r\n",
        "    d_optimizer.step()\r\n",
        "\r\n",
        "    g_optimizer.zero_grad()\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "    #print(real_images.shape)\r\n",
        "\r\n",
        "    \r\n",
        "    #z = torch.from_numpy(z).float()\r\n",
        "\r\n",
        "    \r\n",
        "\r\n",
        "    \r\n",
        "    #print(fake_images.shape)\r\n",
        "\r\n",
        "    fake_images_initial_reconstruction = generator(z , ages)\r\n",
        "\r\n",
        "\r\n",
        "    z_star = identity_preserving_optimizer(z)\r\n",
        "    #z = np.random.uniform(-1, 1, size=(batch_size, z_size))\r\n",
        "    #z = torch.from_numpy(z).float()\r\n",
        "    z_star = z_star.to(device)\r\n",
        "    fake_images_optimized = generator(z_star, ages)\r\n",
        "\r\n",
        "    D_fake = discriminator(fake_images_optimized, ages)\r\n",
        "    g_loss = real_loss(D_fake)\r\n",
        "    \r\n",
        "\r\n",
        "    g_loss.backward()\r\n",
        "    g_optimizer.step()\r\n",
        "\r\n",
        "\r\n",
        "    if batch_i % print_every == 0:\r\n",
        "\r\n",
        "        losses.append((d_loss.item(), g_loss.item()))\r\n",
        "  \r\n",
        "        print('Epoch [{:5d}/{:5d}] | d_loss: {:6.4f} | g_loss: {:6.4f}'.format(\r\n",
        "                epoch+1, num_epochs, d_loss.item(), g_loss.item()))\r\n",
        "        \r\n",
        "\r\n",
        "  generator.eval() \r\n",
        "  fixed_z = fixed_z.to(device)\r\n",
        "  fixed_y = fixed_y.to(device)\r\n",
        "  samples_z = generator(fixed_z, fixed_y)\r\n",
        "  samples.append(samples_z)\r\n",
        "  generator.train() \r\n",
        "  \r\n",
        "\r\n",
        "  checkpoint(generator, discriminator , epoch, model, root_dir)\r\n",
        "\r\n",
        "\r\n",
        "save_samples_ages(samples, fixed_y, model, root_dir)\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QhzuaQKyX8id"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}