{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Self_Attention_GAN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNgBeRzp9MhiKTCcSdFCzoo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VedantDere0104/GANs/blob/main/Self_Attention_GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3GrDUD3Zshu"
      },
      "source": [
        "####"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwZnd2ocaHX5"
      },
      "source": [
        "import torch\r\n",
        "from torch import nn\r\n",
        "import torch.nn.functional as F\r\n",
        "from torch.autograd import Variable\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "\r\n",
        "from torch.optim.optimizer import Optimizer, required\r\n",
        "from torch import Tensor\r\n",
        "from torch.nn import Parameter"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihUsiU-ZcUoF"
      },
      "source": [
        "def l2normalize(v, eps=1e-12):\r\n",
        "  return v / (v.norm() + eps)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpyJiQp5aZFB"
      },
      "source": [
        "class SpectralNorm(nn.Module):\r\n",
        "  def __init__(self , module , name = 'weight' , power_iteration = 1):\r\n",
        "    super(SpectralNorm , self).__init__()\r\n",
        "    self.module = module\r\n",
        "    self.name = name\r\n",
        "    self.power_iteration = power_iteration\r\n",
        "    if not self._made_params():\r\n",
        "      self._make_params()\r\n",
        "  \r\n",
        "  def _update_u_v(self):\r\n",
        "    u = getattr(self.module , self.name + '_u')\r\n",
        "    v = getattr(self.module , self.name + '_v')\r\n",
        "    w = getattr(self.module , self.name + '_bar')\r\n",
        "    height = w.data.shape[0]\r\n",
        "    for _ in range(self.power_iterations):\r\n",
        "      v.data = l2normalize(torch.mv(torch.t(w.view(height,-1).data), u.data))\r\n",
        "      u.data = l2normalize(torch.mv(w.view(height,-1).data, v.data))\r\n",
        "\r\n",
        "    # sigma = torch.dot(u.data, torch.mv(w.view(height,-1).data, v.data))\r\n",
        "    sigma = u.dot(w.view(height, -1).mv(v))\r\n",
        "    setattr(self.module, self.name, w / sigma.expand_as(w))   \r\n",
        "\r\n",
        "  def _made_params(self):\r\n",
        "    try :\r\n",
        "      u = getattr(self.module, self.name + \"_u\")\r\n",
        "      v = getattr(self.module, self.name + \"_v\")\r\n",
        "      w = getattr(self.module, self.name + \"_bar\")\r\n",
        "    except AttributeError:\r\n",
        "      return False\r\n",
        "\r\n",
        "  def _make_params(self):\r\n",
        "    w = getattr(self.module, self.name)\r\n",
        "\r\n",
        "    height = w.data.shape[0]\r\n",
        "    width = w.view(height, -1).data.shape[1]\r\n",
        "\r\n",
        "    u = Parameter(w.data.new(height).normal_(0, 1), requires_grad=False)\r\n",
        "    v = Parameter(w.data.new(width).normal_(0, 1), requires_grad=False)\r\n",
        "    u.data = l2normalize(u.data)\r\n",
        "    v.data = l2normalize(v.data)\r\n",
        "    w_bar = Parameter(w.data)\r\n",
        "\r\n",
        "    del self.module._parameters[self.name]\r\n",
        "\r\n",
        "    self.module.register_parameter(self.name + \"_u\", u)\r\n",
        "    self.module.register_parameter(self.name + \"_v\", v)\r\n",
        "    self.module.register_parameter(self.name + \"_bar\", w_bar)\r\n",
        "\r\n",
        "  def forward(self, *args):\r\n",
        "    self._update_u_v()\r\n",
        "    return self.module.forward(*args)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbUdnFhSbuba"
      },
      "source": [
        "class self_attention(nn.Module):\r\n",
        "  def __init__(self , in_dim , activation):\r\n",
        "    super(self_attention , self).__init__()\r\n",
        "    self.in_dim = in_dim\r\n",
        "    self.activation = activation\r\n",
        "\r\n",
        "    self.query_conv = nn.Conv2d(in_channels=in_dim , out_channels=in_dim //8 , kernel_size=1)\r\n",
        "    self.key_conv = nn.Conv2d(in_channels=in_dim , out_channels=in_dim //8 , kernel_size=1)\r\n",
        "    self.value_conv = nn.Conv2d(in_channels=in_dim , out_channels=in_dim , kernel_size=1)\r\n",
        "    self.gamma = nn.Parameter(torch.zeros(1))\r\n",
        "    self.softmax = nn.Softmax(dim=-1)\r\n",
        "\r\n",
        "  def forward(self , x):\r\n",
        "    '''\r\n",
        "    x: input_feature_map (B , C , W , H)\r\n",
        "    out : self_attention_value + input_feature\r\n",
        "    attention: B X N X N (N is Width*Height)\r\n",
        "    '''\r\n",
        "    m_batchsize , c , width , height = x.size()\r\n",
        "    proj_query = self.query_conv(x).view(m_batchsize , -1 , width * height).permute(0 , 2 , 1) # (B , C , N)\r\n",
        "    proj_key = self.key_conv(x).view(m_batchsize , -1 , width * height) # (B , C , W*H)\r\n",
        "    energy = torch.bmm(proj_query , proj_key)\r\n",
        "    attention = self.softmax(energy)\r\n",
        "    proj_value = self.value_conv(x).view(m_batchsize , -1 , width * height) # (B , C , N)\r\n",
        "\r\n",
        "    out = torch.bmm(proj_value , attention.permute(0 , 2 , 1))\r\n",
        "    out = out.view(m_batchsize , C , width , height)\r\n",
        "    out = self.gamma * out + x\r\n",
        "    return out , attention\r\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PQPzZ1agbfk"
      },
      "source": [
        "class Generator(nn.Module):\r\n",
        "  def __init__(self , batch_size , image_size = 64 , z_dim = 100 , conv_dim = 64):\r\n",
        "    super(Generator , self).__init__()\r\n",
        "    self.image_size = image_size\r\n",
        "    layer1 = []\r\n",
        "    layer2 = []\r\n",
        "    layer3 = []\r\n",
        "    last = []\r\n",
        "\r\n",
        "    repeat_num = int(np.log2(self.image_size)) - 3\r\n",
        "    mult = 2 ** repeat_num # 8\r\n",
        "\r\n",
        "    layer1.append(SpectralNorm(nn.ConvTranspose2d(z_dim , conv_dim *  mult , 4)))\r\n",
        "    layer1.append(nn.BatchNorm2d(conv_dim * mult))\r\n",
        "    layer1.append(nn.ReLU())\r\n",
        "\r\n",
        "    cur_dim = conv_dim * mult\r\n",
        "\r\n",
        "    layer2.append(SpectralNorm(nn.ConvTranspose2d(cur_dim , int(cur_dim / 2) , 4 , 2 , 1)))\r\n",
        "    layer2.append(nn.BatchNorm2d(int(cur_dim / 2)))\r\n",
        "    layer2.append(nn.ReLU())\r\n",
        "\r\n",
        "    cur_dim = int(cur_dim /2)\r\n",
        "\r\n",
        "\r\n",
        "    layer3.append(SpectralNorm(nn.ConvTranspose2d(cur_dim , int(cur_dim / 2) , 4 , 2 , 1)))\r\n",
        "    layer3.append(nn.BatchNorm2d(int(cur_dim / 2)))\r\n",
        "    layer3.append(nn.ReLU())\r\n",
        "\r\n",
        "    if self.image_size == 64:\r\n",
        "      layer4 = []\r\n",
        "      cur_dim = int(cur_dim / 2)\r\n",
        "\r\n",
        "      layer4.append(SpectralNorm(nn.ConvTranspose2d(cur_dim , int(cur_dim / 2) , 4 , 2 , 1)))\r\n",
        "      layer4.append(nn.BatchNorm2d(int(cur_dim / 2)))\r\n",
        "      layer4.append(nn.ReLU())\r\n",
        "\r\n",
        "      self.l4 = nn.Sequential(*layer4)\r\n",
        "\r\n",
        "      cur_dim = int(cur_dim / 2)\r\n",
        "    self.l1 = nn.Sequential(*layer1)\r\n",
        "    self.l2 = nn.Sequential(*layer2)\r\n",
        "    self.l3 = nn.Sequential(*layer3)\r\n",
        "\r\n",
        "    last.append(nn.ConvTranspose2d(cur_dim , 3 , 4 , 2 , 1))\r\n",
        "    last.append(nn.Tanh())\r\n",
        "    self.last = nn.Sequential(*last)\r\n",
        "\r\n",
        "    self.attn1 = self_attention(128 , 'relu')\r\n",
        "    self.attn2 = self_attention(64 , 'relu')\r\n",
        "\r\n",
        "  def forward(self , z):\r\n",
        "    z = z.view(z.size(0), z.size(1), 1, 1)\r\n",
        "    out = self.l1(z)\r\n",
        "    out = self.l2(out)\r\n",
        "    out = self.l3(out)\r\n",
        "    out , p1 = self.attn1(out)\r\n",
        "    out = self.l4(out)\r\n",
        "    out , p2 = self.attn2(out)\r\n",
        "    out = self.last(out)\r\n",
        "\r\n",
        "    return out , p1 , p2\r\n"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zw0zIjsmj-ob"
      },
      "source": [
        "class Discriminator(nn.Module):\r\n",
        "  def __init__(self , batch_size = 64 , image_size = 64 , conv_dim = 64):\r\n",
        "    super(Discriminator , self).__init__()\r\n",
        "    self.image_size = image_size\r\n",
        "\r\n",
        "    layer1 = []\r\n",
        "    layer2 = []\r\n",
        "    layer3 = []\r\n",
        "    last = []\r\n",
        "\r\n",
        "    layer1.append(SpectralNorm(nn.Conv2d(3 , conv_dim , 4 , 2 , 1)))\r\n",
        "    layer1.append(nn.LeakyReLU(0.1))\r\n",
        "\r\n",
        "    cur_dim = conv_dim\r\n",
        "\r\n",
        "    layer2.append(SpectralNorm(nn.Conv2d(cur_dim , cur_dim * 2 , 4 , 2 , 1)))\r\n",
        "    layer2.append(nn.LeakyReLU(0.1))\r\n",
        "\r\n",
        "    cur_dim = cur_dim * 2\r\n",
        "\r\n",
        "    layer3.append(SpectralNorm(nn.Conv2d(cur_dim , cur_dim * 2 , 4 , 2 , 1)))\r\n",
        "    layer3.append(nn.LeakyReLU(0.1))\r\n",
        "\r\n",
        "    cur_dim = cur_dim * 2\r\n",
        "\r\n",
        "    if self.image_size == 64:\r\n",
        "      layer4 = []\r\n",
        "\r\n",
        "      layer4.append(SpectralNorm(nn.Conv2d(cur_dim , cur_dim * 2 , 4 , 2 , 1)))\r\n",
        "      layer4.append(nn.LeakyReLU(0.1))\r\n",
        "\r\n",
        "      self.l4 = nn.Sequential(*layer4)\r\n",
        "      \r\n",
        "      cur_dim = cur_dim * 2\r\n",
        "    self.l1 = nn.Sequential(*layer1)\r\n",
        "    self.l2 = nn.Sequential(*layer2)\r\n",
        "    self.l3 = nn.Sequential(*layer3)\r\n",
        "\r\n",
        "    last.append(nn.Conv2d(cur_dim , 1 , 4))\r\n",
        "    self.last = nn.Sequential(*last)\r\n",
        "\r\n",
        "    self.attn1 = self_attention(256 , 'relu')\r\n",
        "    self.attn2 = self_attention(512 , 'relu')\r\n",
        "\r\n",
        "  def forward(self , x):\r\n",
        "    out = self.l1(x)\r\n",
        "    out = self.l2(out)\r\n",
        "    out = self.l3(out)\r\n",
        "    out , p1 = self.attn1(out)\r\n",
        "    out = self.l4(out)\r\n",
        "    out , p2 = self.attn2(out)\r\n",
        "    out = self.last(out)\r\n",
        "    \r\n",
        "    return out , p1 , p2\r\n",
        "\r\n"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fMXQ4jioSnu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}