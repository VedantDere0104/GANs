{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Image_translation_with_dual‐directional_generative_adversarial_networks.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1EjP9jSREdEv1WNEEQ2yLIwWVwdrRVyad",
      "authorship_tag": "ABX9TyNwTIKiv3dVd1WrsUkFLPMf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VedantDere0104/GANs/blob/main/Image_translation_with_dual%E2%80%90directional_generative_adversarial_networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HX0EPXGet-P_"
      },
      "source": [
        "Image translation with dual‐directional generative adversarial\r\n",
        "networks :- https://ietresearch.onlinelibrary.wiley.com/doi/full/10.1049/cvi2.12011"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rV10fiytyMS"
      },
      "source": [
        "####"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "teqC5zTYuIDV"
      },
      "source": [
        "import torch\r\n",
        "from torch import nn\r\n",
        "from torchsummary import summary\r\n",
        "from torchvision import transforms\r\n",
        "from torch.utils.data import DataLoader\r\n",
        "from tqdm.auto import tqdm\r\n",
        "from torchvision.utils import make_grid\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import torchvision"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RgKEmhlFXUw0"
      },
      "source": [
        "def show_tensor_images(image_tensor, num_images=2, size=(1, 28, 28) , switch = True):\r\n",
        "  image_shifted = image_tensor\r\n",
        "  #print(image_shifted)\r\n",
        "  image_unflat = image_shifted.detach().cpu().view(-1, *size)\r\n",
        "  #print(image_unflat)\r\n",
        "  image_grid = make_grid(image_unflat[:num_images], nrow=2 , normalize=False)\r\n",
        "  #print(image_grid)\r\n",
        "  if switch:\r\n",
        "    image_grid = image_grid * 255.0\r\n",
        "  plt.imshow(image_grid.permute(1, 2, 0).squeeze())\r\n",
        "  plt.show()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOcW-5AyXhLq"
      },
      "source": [
        "def crop(image, new_shape):\r\n",
        "    middle_height = image.shape[2] // 2\r\n",
        "    middle_width = image.shape[3] // 2\r\n",
        "    starting_height = middle_height - new_shape[2] // 2\r\n",
        "    final_height = starting_height + new_shape[2]\r\n",
        "    starting_width = middle_width - new_shape[3] // 2\r\n",
        "    final_width = starting_width + new_shape[3]\r\n",
        "    cropped_image = image[:, :, starting_height:final_height, starting_width:final_width]\r\n",
        "    return cropped_image"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZO7QM3ZWk-I"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTMMnb22Ucsf"
      },
      "source": [
        "class Helper_1(nn.Module):\r\n",
        "  def __init__(self , in_channels , out_channels , kernel_size = (2 , 2) , stride = (2 , 2) , use_batch_norm = True):\r\n",
        "    super(Helper_1 , self).__init__()\r\n",
        "\r\n",
        "    self.use_batch_norm = use_batch_norm\r\n",
        "    self.conv1 = nn.Conv2d(in_channels , out_channels , kernel_size , stride)\r\n",
        "    self.batch_norm = nn.InstanceNorm2d(out_channels)\r\n",
        "    self.relu = nn.ReLU()\r\n",
        "\r\n",
        "  def forward(self , x):\r\n",
        "    #print(x.shape)\r\n",
        "    x = self.conv1(x)\r\n",
        "    x = self.batch_norm(x)\r\n",
        "    x = self.relu(x)\r\n",
        "    #print(x.shape)\r\n",
        "    return x\r\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5Aj3PdsVGRH"
      },
      "source": [
        "helper = Helper_1(3 , 64 , (2 , 2) , (2 , 2) , True)\r\n",
        "x = torch.randn(1 , 3 , 64 , 64)\r\n",
        "z = helper(x)\r\n",
        "z.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5IsD5v-eYKt"
      },
      "source": [
        "class Encoder(nn.Module):\r\n",
        "  def __init__(self , in_channels , hidden_dim , out_channels):\r\n",
        "    super(Encoder , self).__init__()\r\n",
        "\r\n",
        "    self.conv1 = Helper_1(in_channels , hidden_dim , (2 , 2) , (2 , 2) , False)\r\n",
        "    self.conv2 = Helper_1(hidden_dim , hidden_dim * 2 , (2 , 2) , (2 , 2) , True)\r\n",
        "    self.conv3 = Helper_1(hidden_dim * 2 , hidden_dim * 4 , (2 , 2) , (2 , 2) , True)\r\n",
        "    self.conv4 = Helper_1(hidden_dim * 4 , hidden_dim * 8 , (2 , 2) , (2 , 2) ,True)\r\n",
        "    self.conv5 = Helper_1(hidden_dim * 8 , hidden_dim * 16 , (2 , 2) , (2 ,2) , True)\r\n",
        "    self.conv6 = Helper_1(hidden_dim * 16 , hidden_dim * 32 , (2 , 2) ,(2 , 2) , True)\r\n",
        "    self.conv7 = Helper_1(hidden_dim * 32 , hidden_dim * 64 , (2 , 2) , (2 , 2) , True)\r\n",
        "    self.conv8 = Helper_1(hidden_dim * 64 , hidden_dim * 32 , (2 , 2) , (2 , 2) , True)\r\n",
        "    self.flatten = nn.Flatten()\r\n",
        "    self.linear1 = nn.Linear(4096 , hidden_dim * 16)\r\n",
        "    self.batchnorm = nn.BatchNorm1d(hidden_dim * 16)\r\n",
        "    self.relu = nn.ReLU()\r\n",
        "    \r\n",
        "    self.linear2 = nn.Linear(hidden_dim * 16 , hidden_dim * 16)\r\n",
        "\r\n",
        "\r\n",
        "  def forward(self , x):\r\n",
        "    x = self.conv1(x)\r\n",
        "    x = self.conv2(x) \r\n",
        "    x = self.conv3(x)\r\n",
        "    x = self.conv4(x) \r\n",
        "    x = self.conv5(x)\r\n",
        "    x = self.conv6(x)\r\n",
        "    x = self.conv7(x)\r\n",
        "    x = self.conv8(x)\r\n",
        "    x = self.flatten(x)\r\n",
        "    x = self.relu(self.batchnorm(self.linear1(x)))\r\n",
        "    x = self.linear2(x)\r\n",
        "    x = x.view(x.shape[0] , x.shape[1] , 1 , 1)\r\n",
        "    return x"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yzPE2OTqUWl7"
      },
      "source": [
        "encoder = Encoder(3 , 32 , 512).to(device)\r\n",
        "summary(encoder , (3 , 512 , 512))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gTxc0FaNYZpT"
      },
      "source": [
        "x = torch.randn(10 , 3 , 512 , 512 , device = device)\r\n",
        "z = encoder(x)\r\n",
        "z.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXOYv0GDbdQ4"
      },
      "source": [
        "class Helper_2(nn.Module):\r\n",
        "  def __init__(self , in_channels , out_channels , kernel_size = (2 , 2) , stride = (2 , 2) , use_batchnorm = True):\r\n",
        "    super(Helper_2 , self).__init__()\r\n",
        "\r\n",
        "    self.use_batchnorm = use_batchnorm\r\n",
        "    self.convT1 = nn.ConvTranspose2d(in_channels , out_channels , \r\n",
        "                                     kernel_size , stride)\r\n",
        "    \r\n",
        "    if self.use_batchnorm:\r\n",
        "      self.batchnorm = nn.InstanceNorm2d(out_channels)\r\n",
        "    self.lrelu = nn.LeakyReLU()\r\n",
        "\r\n",
        "  def forward(self , x):\r\n",
        "    x = self.convT1(x)\r\n",
        "    if self.use_batchnorm:\r\n",
        "      x = self.batchnorm(x)\r\n",
        "    x = self.lrelu(x)\r\n",
        "    return x\r\n",
        "    "
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cE7k-5pAcBBz"
      },
      "source": [
        "helper_2 = Helper_2(3 , 32 , (2 , 2) , (2 , 2) , True).to(device)\r\n",
        "summary(helper_2 , (3 , 512 , 512))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRr7IWTpEBet"
      },
      "source": [
        "class Generator(nn.Module):\r\n",
        "  def __init__(self , z_in_channels , img_in_channels , hidden_dim , out_channels):\r\n",
        "    super(Generator , self).__init__()\r\n",
        "\r\n",
        "    self.encoder = Encoder(3 , 32 , 512)\r\n",
        "\r\n",
        "    self.convT1 = Helper_2(z_in_channels , hidden_dim , use_batchnorm=False)\r\n",
        "    self.convT2 = Helper_2(hidden_dim , hidden_dim  *2)\r\n",
        "    self.convT3 = Helper_2(hidden_dim * 2 , hidden_dim * 4)\r\n",
        "    self.convT4 = Helper_2(hidden_dim * 4 , hidden_dim * 8)\r\n",
        "    self.convT5 = Helper_2(hidden_dim * 8 , hidden_dim * 16)\r\n",
        "    self.convT6 = Helper_2(hidden_dim * 16 , hidden_dim  * 32)\r\n",
        "    self.convT7 = Helper_2(hidden_dim * 32 , hidden_dim * 32)\r\n",
        "\r\n",
        "    self.conv1 = Helper_1(img_in_channels , hidden_dim , use_batch_norm=False)\r\n",
        "    self.conv2 = Helper_1(hidden_dim , hidden_dim * 32)\r\n",
        "\r\n",
        "    self.convT_1 = Helper_2(hidden_dim * 32 * 2 , hidden_dim * 32 , use_batchnorm=False)\r\n",
        "    self.convT_2 = Helper_2(hidden_dim * 32 , 3)\r\n",
        "\r\n",
        "\r\n",
        "  def forward(self , x , y):\r\n",
        "    x = self.encoder(x)\r\n",
        "    x = self.convT1(x)\r\n",
        "    x = self.convT2(x)\r\n",
        "    x = self.convT3(x)\r\n",
        "    x = self.convT4(x)\r\n",
        "    x = self.convT5(x)\r\n",
        "    x = self.convT6(x)\r\n",
        "    x = self.convT7(x)\r\n",
        "\r\n",
        "    y = self.conv1(y)\r\n",
        "    y = self.conv2(y)\r\n",
        "\r\n",
        "    z = torch.cat([x , y] , dim=1)\r\n",
        "\r\n",
        "    z = self.convT_1(z)\r\n",
        "    z = self.convT_2(z)\r\n",
        "\r\n",
        "    return z\r\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLl-cOorbcjf"
      },
      "source": [
        "generator = Generator(512 , 3 , 32 , 3).to(device)\r\n",
        "x = torch.randn(5 , 3 , 512 , 512 , device = device)\r\n",
        "ans = generator(x , x)\r\n",
        "ans.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wRaq5qPdn56"
      },
      "source": [
        "class Discriminator(nn.Module):\r\n",
        "  def __init__(self , in_channels , hidden_dim , out_channels):\r\n",
        "    super(Discriminator , self).__init__()\r\n",
        "\r\n",
        "    self.conv1 = Helper_1(in_channels , hidden_dim , use_batch_norm=False)\r\n",
        "    self.conv2 = Helper_1(hidden_dim , hidden_dim * 2)\r\n",
        "    self.conv3 = Helper_1(hidden_dim * 2 , hidden_dim * 4)\r\n",
        "    self.conv4 = Helper_1(hidden_dim * 4 , hidden_dim * 8)\r\n",
        "    self.conv5 = Helper_1(hidden_dim * 8 , hidden_dim * 16)\r\n",
        "    self.conv6 = Helper_1(hidden_dim * 16 , hidden_dim * 32)\r\n",
        "    self.conv7 = Helper_1(hidden_dim * 32 , hidden_dim * 32)\r\n",
        "    self.conv8 = Helper_1(hidden_dim * 32 , hidden_dim * 32)\r\n",
        "    self.flatten = nn.Flatten()\r\n",
        "\r\n",
        "    self.relu = nn.ReLU()\r\n",
        "    self.linear1 = nn.Linear(4096 , hidden_dim * 32)\r\n",
        "    self.batchnorm1 = nn.BatchNorm1d(hidden_dim * 32)\r\n",
        "    self.linear2 = nn.Linear(hidden_dim * 32 , hidden_dim * 8)\r\n",
        "    self.batchnorm2 = nn.BatchNorm1d(hidden_dim * 8)\r\n",
        "    self.linear3 = nn.Linear(hidden_dim * 8 , hidden_dim)\r\n",
        "    self.batchnorm3 = nn.BatchNorm1d(hidden_dim)\r\n",
        "    self.linear4 = nn.Linear(hidden_dim , out_channels)\r\n",
        "    self.sigmoid = nn.Sigmoid()\r\n",
        "\r\n",
        "\r\n",
        "  def forward(self , x):\r\n",
        "    x = self.conv1(x)\r\n",
        "    x = self.conv2(x)\r\n",
        "    x = self.conv3(x)\r\n",
        "    x = self.conv4(x)\r\n",
        "    x = self.conv5(x)\r\n",
        "    x = self.conv6(x)\r\n",
        "    x = self.conv7(x)\r\n",
        "    x = self.conv8(x)\r\n",
        "    x = self.flatten(x)\r\n",
        "    x = self.relu(self.batchnorm1(self.linear1(x)))\r\n",
        "    x = self.relu(self.batchnorm2(self.linear2(x)))\r\n",
        "    x = self.relu(self.batchnorm3(self.linear3(x)))\r\n",
        "    x = self.sigmoid(self.linear4(x))\r\n",
        "    return x"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Utk0TJRMED4"
      },
      "source": [
        "disc = Discriminator(3 , 32 , 1).to(device)\r\n",
        "summary(disc , (3 , 512 , 512))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-Fx-VQxixZ6"
      },
      "source": [
        "a = torch.randn(10 , 3 , 512 , 512 ,device =  device)\r\n",
        "a = disc(a)\r\n",
        "print(a.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KEnF9OgMqWC"
      },
      "source": [
        "generator_x = Generator(512 , 3 , 32 , 3).to(device)\r\n",
        "generator_y = Generator(512 , 3 , 32 , 3).to(device)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptVmv4cbWgQd"
      },
      "source": [
        "discriminator_x = Discriminator(3 , 32 , 1).to(device)\r\n",
        "discriminator_y = Discriminator(3 , 32 , 1).to(device)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRQ7NosFFVqM"
      },
      "source": [
        "def weights_init(m):\r\n",
        "  if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\r\n",
        "    torch.nn.init.normal_(m.weight, 0.0, 0.02)\r\n",
        "  if isinstance(m, nn.BatchNorm2d):\r\n",
        "    torch.nn.init.normal_(m.weight, 0.0, 0.02)\r\n",
        "    torch.nn.init.constant_(m.bias, 0)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xUqI_pxFgCX"
      },
      "source": [
        "generator_x = generator_x.apply(weights_init)\r\n",
        "generator_y = generator_y.apply(weights_init)\r\n",
        "discriminator_x = discriminator_x.apply(weights_init)\r\n",
        "discriminator_y = discriminator_y.apply(weights_init)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftPDqlaRWqhH"
      },
      "source": [
        "criterion = nn.BCEWithLogitsLoss()\r\n",
        "l1_loss = nn.L1Loss()\r\n",
        "lambda_recon = 200"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_EDOj2FYW3XJ"
      },
      "source": [
        "n_epochs = 100\r\n",
        "input_dim = 3\r\n",
        "real_dim = 3\r\n",
        "display_step = 10\r\n",
        "batch_size = 2\r\n",
        "lr = 0.000002\r\n",
        "target_shape = 512"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGhMEkvffdLm"
      },
      "source": [
        "transform = transforms.Compose([ transforms.ToTensor(), ])\r\n",
        "\r\n",
        "dataset = torchvision.datasets.ImageFolder(\"/content/drive/MyDrive/Maps/maps/\", transform=transform)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vm2vUsksMOYw"
      },
      "source": [
        "dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nh1ONoH_Xy_n"
      },
      "source": [
        "  mean_generator_loss = 0\r\n",
        "  mean_discriminator_loss = 0\r\n",
        "  dataloader = DataLoader(dataset , batch_size = batch_size , shuffle=True)\r\n",
        "  cur_step = 0"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dU4hSN86n9t7"
      },
      "source": [
        "betas = (0.5 , 0.999)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhOz_fhdYIYj"
      },
      "source": [
        "opt_generator_y = torch.optim.Adam(generator_y.parameters() , lr=lr , betas=betas)\r\n",
        "opt_generator_x = torch.optim.Adam(generator_x.parameters() , lr = lr , betas = betas)\r\n",
        "opt_discriminator_x = torch.optim.Adam(discriminator_x.parameters() , lr = lr , betas=betas)\r\n",
        "opt_discriminator_y = torch.optim.Adam(discriminator_y.parameters() , lr = lr , betas=betas)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Msewq30ZXFo"
      },
      "source": [
        "def get_gen_loss(gen , disc , real , condition , adv_criterion , recon_criterion , lambda_recon):\r\n",
        "  fake = gen(condition)\r\n",
        "  disc_fake_hat = disc(fake , condition)\r\n",
        "  gen_adv_loss = adv_criterion(disc_fake_hat , torch.ones_like(disc_fake_hat))\r\n",
        "  gen_rec_loss = recon_criterion(real , condition)\r\n",
        "  gen_loss = gen_adv_loss + lambda_recon * gen_rec_loss\r\n",
        "  return gen_loss\r\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zh-cbI2-SOXn"
      },
      "source": [
        "def get_loss(fake , real  , criterion = criterion , l1_loss = l1_loss , lambda_recon = lambda_recon):\r\n",
        "  gen_loss = criterion(fake , real)\r\n",
        "  l1_loss = l1_loss(fake , real)\r\n",
        "  loss = gen_loss + lambda_recon *  l1_loss\r\n",
        "  return loss"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8u3uKZxdXDut"
      },
      "source": [
        "for epoch in range(n_epochs):\r\n",
        "  for img , _ in tqdm(dataloader):\r\n",
        "    image_width = img.shape[3]\r\n",
        "    condition = img[: , : , : , :image_width//2]\r\n",
        "    condition = nn.functional.interpolate(condition , size = target_shape)\r\n",
        "    real = img[: , : , : , image_width//2:]\r\n",
        "    real = nn.functional.interpolate(real , size = target_shape)\r\n",
        "    cur_batch_size = len(condition)\r\n",
        "    real = real.to(device)\r\n",
        "    condition = condition.to(device)  \r\n",
        "\r\n",
        "    opt_generator_y.zero_grad()\r\n",
        "    Y_XY = generator_y(real , real)\r\n",
        "    Y_YY = generator_y(real , condition)\r\n",
        "\r\n",
        "    #loss_Y_XY = criterion(Y_XY , condition)\r\n",
        "    loss_Y_XY = get_loss(Y_XY , condition)\r\n",
        "    #loss_Y_YY = criterion(Y_YY , condition)\r\n",
        "    loss_Y_YY = get_loss(Y_YY , condition)\r\n",
        "    loss_Y = (loss_Y_XY + loss_Y_YY) /2\r\n",
        "\r\n",
        "    loss_Y.backward()\r\n",
        "    opt_generator_y.step()\r\n",
        "\r\n",
        "\r\n",
        "    opt_generator_x.zero_grad()\r\n",
        "    X_XX = generator_x(condition , real)\r\n",
        "    X_YX = generator_x(condition , condition)\r\n",
        "\r\n",
        "    #loss_X_XX = criterion(X_XX , real)\r\n",
        "    loss_X_XX = get_loss(X_XX , real)\r\n",
        "    #loss_X_YX = criterion(X_YX , real)\r\n",
        "    loss_X_YX = get_loss(X_YX , real)\r\n",
        "    loss_X = (loss_X_XX + loss_X_YX) /2\r\n",
        "    loss_X.backward()\r\n",
        "    opt_generator_x.step()\r\n",
        "\r\n",
        "\r\n",
        "    opt_discriminator_y.zero_grad()\r\n",
        "    with torch.no_grad():\r\n",
        "      disc_Y_XY = generator_y(real , real)\r\n",
        "      disc_Y_YY = generator_y(condition ,real)\r\n",
        "    disc_fake_y_pred_YX = discriminator_y(disc_Y_XY)\r\n",
        "    disc_loss_fake_pred_YX = criterion(disc_fake_y_pred_YX , torch.zeros_like(disc_fake_y_pred_YX))\r\n",
        "\r\n",
        "    disc_fake_y_pred_YY = discriminator_y(disc_Y_YY)\r\n",
        "    disc_loss_fake_pred_YY = criterion(disc_fake_y_pred_YY , torch.zeros_like(disc_fake_y_pred_YY))\r\n",
        "\r\n",
        "    disc_real_pred = discriminator_y(condition)\r\n",
        "    disc_real_pred_loss = criterion(disc_real_pred , torch.ones_like(disc_real_pred))\r\n",
        "\r\n",
        "    disc_y_loss = (disc_loss_fake_pred_YX + disc_loss_fake_pred_YY + disc_real_pred_loss) /3\r\n",
        "    #print(disc_y_loss)\r\n",
        "\r\n",
        "    disc_y_loss.backward()\r\n",
        "    opt_discriminator_y.step()\r\n",
        "\r\n",
        "    opt_discriminator_x.zero_grad()\r\n",
        "    with torch.no_grad():\r\n",
        "      disc_X_XX = generator_x(real , condition)\r\n",
        "      disc_X_YX_ = generator_x(condition , condition)\r\n",
        "    disc_fake_pred_XX = discriminator_x(disc_X_XX)\r\n",
        "    disc_loss_fake_pred_XX = criterion(disc_fake_pred_XX , torch.zeros_like(disc_fake_pred_XX))\r\n",
        "\r\n",
        "    disc_fake_pred_YX_ = discriminator_x(disc_X_YX_)\r\n",
        "    disc_loss_fake_pred_YX_ = criterion(disc_fake_pred_YX_ , torch.zeros_like(disc_fake_pred_YX_))\r\n",
        "\r\n",
        "    disc_x_real_pred = discriminator_x(real)\r\n",
        "    disc_x_real_loss = criterion(disc_x_real_pred , torch.ones_like(disc_x_real_pred))\r\n",
        "    disc_x_loss = (disc_loss_fake_pred_XX + disc_loss_fake_pred_YX_ + disc_x_real_loss) / 3\r\n",
        "    #print(disc_x_loss)\r\n",
        "    disc_x_loss.backward()\r\n",
        "    opt_discriminator_x.step()\r\n",
        "\r\n",
        "    disc_loss = (disc_x_loss + disc_y_loss)/2\r\n",
        "    gen_loss = (loss_X + loss_Y)/2\r\n",
        "\r\n",
        "    mean_discriminator_loss += disc_loss.item() / display_step\r\n",
        "    mean_generator_loss += gen_loss.item() / display_step\r\n",
        "\r\n",
        "    if cur_step % display_step == 0:\r\n",
        "      if cur_step > 0:\r\n",
        "        print(f\"Epoch {epoch}: Step {cur_step}: Generator loss: {mean_generator_loss}, Discriminator loss: {mean_discriminator_loss}\")\r\n",
        "      else:\r\n",
        "        print(\"Pretrained initial state\")\r\n",
        "      print('Y')\r\n",
        "      show_tensor_images(condition, size=(input_dim, target_shape, target_shape) , switch=False)\r\n",
        "      print('X')\r\n",
        "      show_tensor_images(real, size=(real_dim, target_shape, target_shape) , switch=False)\r\n",
        "      print('X --> Y')\r\n",
        "      show_tensor_images(Y_XY, size=(real_dim, target_shape, target_shape) , switch=True)\r\n",
        "      #plt.imshow(  Y_XY.permute(1, 2, 0))\r\n",
        "      print('Y --> Y')\r\n",
        "      show_tensor_images(Y_YY, size=(real_dim, target_shape, target_shape) , switch=True)\r\n",
        "      #plt.imshow(  Y_YY.permute(1, 2, 0))\r\n",
        "      print('X --> X')\r\n",
        "      show_tensor_images(X_XX, size=(real_dim, target_shape, target_shape) , switch=False)\r\n",
        "      print('Y --> X')\r\n",
        "      show_tensor_images(X_YX, size=(real_dim, target_shape, target_shape) , switch=False)\r\n",
        "      mean_generator_loss = 0\r\n",
        "      mean_discriminator_loss = 0\r\n",
        "    cur_step += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8B7mGDfeQMS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}